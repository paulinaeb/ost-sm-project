{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871cce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "import json, os, re, datetime, csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4a5e3",
   "metadata": {},
   "source": [
    "### We have two datasets:\n",
    "#### 1) Linkedin jobs ads (dynamic)\n",
    "#### 2) ECSF (static)\n",
    "\n",
    "The first dataset, has to be dynamic in other words, we need to put it in a producer-topic-consumer-casssandra\n",
    "\n",
    "The second dataset, has to be in cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04db98a",
   "metadata": {},
   "source": [
    "### To clean the linkedin job ads dataset:\n",
    "\n",
    "#### 1) delete irrelevent columns\n",
    "#### 2) remove duplicates\n",
    "#### 3) remove null values\n",
    "#### 4) preprocess Skills column\n",
    "#### 5) preprocess Description column\n",
    "#### 6) preprocess Title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01252ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('jobs.json')\n",
    "# number of rows / samples\n",
    "row_count = df.shape[0]\n",
    "print(f\"Number of rows in the dataset: {row_count}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b9bb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Description', 'Primary Description', 'Detail URL', 'Location',\n",
       "       'Skill', 'Insight', 'Job State', 'Poster Id', 'Company Name',\n",
       "       'Company Logo', 'Created At', 'Scraped At'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23e3fe",
   "metadata": {},
   "source": [
    "#### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "670e7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of detected duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# Duplicate check (first 5 columns)\n",
    "subset_cols = df.columns[:5].tolist()\n",
    "initial_rows = df.shape[0]\n",
    "\n",
    "duplicate_count = df.duplicated(subset=subset_cols).sum()\n",
    "print(f\"number of detected duplicates: {duplicate_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb99d58",
   "metadata": {},
   "source": [
    "#### Drop unrelated columns (meaningless columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2ffeefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Description', 'Primary Description', 'Detail URL', 'Location',\n",
       "       'Skill', 'Company Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop hiring manager-related columns, scrape info, non needed columns\n",
    "columns_to_drop = [\n",
    "    'Insight', 'Job State', 'Poster Id','Company Logo', 'Created At', 'Scraped At',\n",
    "]\n",
    "df.drop(columns=columns_to_drop, errors=\"ignore\", inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa78bbd",
   "metadata": {},
   "source": [
    "#### Cleaning Description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c6bbf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> The first row of the Description column before cleaning: \n",
      " Vattenfall IT’s Data Centre department manages on-premises workloads and Public Cloud services under a hybrid strategy. While the Public Cloud Platform Services team handles cloud-based workloads, the Data Centre team oversees country-based Data Centers and is transitioning from traditional hosting to Virtual Private Cloud (VPC) platforms to meet regulatory and business-specific needs.\n",
      "\n",
      "Private Cloud Platform Services\n",
      "\n",
      "A new department within the Data Centre organization manages Private Cloud Pl\n",
      "---------------------------------------------------------\n",
      "-> The first row of the Description column after cleaning: \n",
      " Vattenfall IT’s Data Centre department manages on-premises workloads and Public Cloud services under a hybrid strategy. While the Public Cloud Platform Services team handles cloud-based workloads, the Data Centre team oversees country-based Data Centers and is transitioning from traditional hosting to Virtual Private Cloud (VPC) platforms to meet regulatory and business-specific needs.. . Private Cloud Platform Services. . A new department within the Data Centre organization manages Private Clou\n"
     ]
    }
   ],
   "source": [
    "print('-> The first row of the Description column before cleaning: \\n',df[\"Description\"][1][:500])\n",
    "# 1) remove \\n\n",
    "df[\"Description\"] = df[\"Description\"].str.replace(\"\\n\", \". \", regex=True)\n",
    "\n",
    "\n",
    "# 2) remove unrelated sentences\n",
    "# List of unwanted phrases to remove\n",
    "unwanted_phrases = [\n",
    "    \"job description\", \"job title\", \"role description\", \"about the job\",\n",
    "    \"about the role\", \"about us\", \"about the opportunity\", \"requirements\",\n",
    "    \"job requirements\", \"role requirements\", \"your role\", \"your job\",\n",
    "    \"offer\", \"employment offer\", \"your profile\", \"responsibilities\",\n",
    "    \"job responsibilities\", \"role responsibilities\", \"overview\", \"position overview\", \n",
    "    \"who are we?\", \"who we are\", \"who are we ?\"\n",
    "]\n",
    "# Precompile regex patterns for efficiency\n",
    "patterns = [re.compile(rf\"^{phrase}[:\\s]*\", re.IGNORECASE) for phrase in unwanted_phrases]\n",
    "\n",
    "# Cleaning function\n",
    "def clean_description(text):\n",
    "    if isinstance(text, str):\n",
    "        for pattern in patterns:\n",
    "            text = pattern.sub(\"\", text).strip()  # Remove matched phrase and trim spaces\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# Apply function to the description column\n",
    "\n",
    "df[\"Description\"] = df[\"Description\"].apply(clean_description)\n",
    "print('---------------------------------------------------------')\n",
    "print('-> The first row of the Description column after cleaning: \\n',df[\"Description\"][1][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc06a1b",
   "metadata": {},
   "source": [
    "#### Clean Skills column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "045f6b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> The first row of the Description column before cleaning: \n",
      " Skills: English, Data Centers, +8 more\n",
      "---------------------------------------------------------\n",
      "-> The first row of the Description column after cleaning: \n",
      " English, Data Centers\n"
     ]
    }
   ],
   "source": [
    "print('-> The first row of the Description column before cleaning: \\n',df[\"Skill\"][1][:400])\n",
    "\n",
    "# Preprocess the Skill column\n",
    "def clean_skills(skill_str):\n",
    "    if isinstance(skill_str, str):\n",
    "        # Remove \"Skills: \" if it appears at the beginning\n",
    "        skill_str = re.sub(r\"^Skills:\\s*\", \"\", skill_str).strip()\n",
    "        # Remove \"X of Y skills match your profile - you may be ...\" pattern\n",
    "        skill_str = re.sub(r\"\\d+\\s+of\\s+\\d+\\s+skills match your profile - you may be.*\", \"\", skill_str, flags=re.IGNORECASE).strip()\n",
    "        # Remove \", +X more\" where X is any number\n",
    "        skill_str = re.sub(r\",\\s\\+\\d+\\s+more\", \"\", skill_str).strip()\n",
    "        return skill_str\n",
    "    return \"\"\n",
    "\n",
    "df[\"Skill\"] = df[\"Skill\"].apply(clean_skills)\n",
    "df.head(5)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print('-> The first row of the Description column after cleaning: \\n',df[\"Skill\"][1][:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c33e6",
   "metadata": {},
   "source": [
    "#### Clean Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a54c2926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samiha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Primary Description</th>\n",
       "      <th>Detail URL</th>\n",
       "      <th>Location</th>\n",
       "      <th>Skill</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>inhouse consultant information security awareness</td>\n",
       "      <td>. As a self-financed family company, we are mo...</td>\n",
       "      <td>Deichmann · Essen, North Rhine-Westphalia, Ger...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4300643770</td>\n",
       "      <td>Essen, North Rhine-Westphalia, Germany</td>\n",
       "      <td>Information Security, Phishing</td>\n",
       "      <td>Deichmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>senior storage engineer</td>\n",
       "      <td>Vattenfall IT’s Data Centre department manages...</td>\n",
       "      <td>Vattenfall · Stockholm, Stockholm County, Swed...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4279274865</td>\n",
       "      <td>Stockholm, Stockholm County, Sweden</td>\n",
       "      <td>English, Data Centers</td>\n",
       "      <td>Vattenfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dba</td>\n",
       "      <td>SoSafe has the ambition to become the leading ...</td>\n",
       "      <td>SoSafe · Madrid, Community of Madrid, Spain (O...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4292576067</td>\n",
       "      <td>Madrid, Community of Madrid, Spain</td>\n",
       "      <td>Amazon Web Services (AWS), Amazon Relational D...</td>\n",
       "      <td>SoSafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business system analyst</td>\n",
       "      <td>At Exness, we are not just a leading trading b...</td>\n",
       "      <td>Exness · Limassol, Cyprus (Hybrid)</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4272079301</td>\n",
       "      <td>Limassol, Cyprus</td>\n",
       "      <td>English, Programming Languages</td>\n",
       "      <td>Exness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data privacy protection consultant hmx</td>\n",
       "      <td>In Experites, we seek privacy and data protect...</td>\n",
       "      <td>EXPERTIS SPAIN · Three songs, Community of Mad...</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/4300652034</td>\n",
       "      <td>Tres Cantos, Community of Madrid, Spain</td>\n",
       "      <td>Data Privacy, Data Protection Act</td>\n",
       "      <td>Experis España</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  inhouse consultant information security awareness   \n",
       "1                            senior storage engineer   \n",
       "2                                                dba   \n",
       "3                            business system analyst   \n",
       "4             data privacy protection consultant hmx   \n",
       "\n",
       "                                         Description  \\\n",
       "0  . As a self-financed family company, we are mo...   \n",
       "1  Vattenfall IT’s Data Centre department manages...   \n",
       "2  SoSafe has the ambition to become the leading ...   \n",
       "3  At Exness, we are not just a leading trading b...   \n",
       "4  In Experites, we seek privacy and data protect...   \n",
       "\n",
       "                                 Primary Description  \\\n",
       "0  Deichmann · Essen, North Rhine-Westphalia, Ger...   \n",
       "1  Vattenfall · Stockholm, Stockholm County, Swed...   \n",
       "2  SoSafe · Madrid, Community of Madrid, Spain (O...   \n",
       "3                 Exness · Limassol, Cyprus (Hybrid)   \n",
       "4  EXPERTIS SPAIN · Three songs, Community of Mad...   \n",
       "\n",
       "                                      Detail URL  \\\n",
       "0  https://www.linkedin.com/jobs/view/4300643770   \n",
       "1  https://www.linkedin.com/jobs/view/4279274865   \n",
       "2  https://www.linkedin.com/jobs/view/4292576067   \n",
       "3  https://www.linkedin.com/jobs/view/4272079301   \n",
       "4  https://www.linkedin.com/jobs/view/4300652034   \n",
       "\n",
       "                                  Location  \\\n",
       "0   Essen, North Rhine-Westphalia, Germany   \n",
       "1      Stockholm, Stockholm County, Sweden   \n",
       "2       Madrid, Community of Madrid, Spain   \n",
       "3                         Limassol, Cyprus   \n",
       "4  Tres Cantos, Community of Madrid, Spain   \n",
       "\n",
       "                                               Skill    Company Name  \n",
       "0                     Information Security, Phishing       Deichmann  \n",
       "1                              English, Data Centers      Vattenfall  \n",
       "2  Amazon Web Services (AWS), Amazon Relational D...          SoSafe  \n",
       "3                     English, Programming Languages          Exness  \n",
       "4                  Data Privacy, Data Protection Act  Experis España  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1) download the stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# 2) Load stopwords for each detected language\n",
    "stop_words = set()\n",
    "languages = [\"english\", \"german\", \"dutch\", \"italian\", \"spanish\", \"french\", \"portuguese\"]\n",
    "# Add stopwords for each language to the set\n",
    "for lang in languages:\n",
    "    stop_words.update(set(stopwords.words(lang)))\n",
    "# Add custom stopwords to the set\n",
    "custom_stopwords = {\"mwd\", \"mfd\", \"gender\", \"fh\", \"mf\", \"hf\", \"fmd\", \"wmd\", \"etc\", \"M/F\"}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# 3) Function to clean stop words from text \n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "        text = \" \".join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# Clean \"Title\" and \"Description\" columns\n",
    "df[\"Title\"] = df[\"Title\"].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7fea73",
   "metadata": {},
   "source": [
    "#### Clean Location column and add Country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da43fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. mapping dictionary\n",
    "# ------------------------------------------------------------------\n",
    "df[\"Location\"] = df[\"Location\"].replace({\n",
    "\n",
    "    \"England Metropolitan Area\": \"United Kingdom\",\n",
    "    \"Brno Metropolitan Area\": \"Czechia\", \"Plzen Metropolitan Area\": \"Czechia\", \"Prague Metropolitan Area\": \"Czechia\",\n",
    "    \"Sofia Metropolitan Area\": \"Bulgaria\", \"Plovdiv Metropolitan Area\": \"Bulgaria\",\n",
    "    \"Athens Metropolitan Area\": \"Greece\", \"Thessaloniki Metropolitan Area\": \"Greece\",\n",
    "    \"Geneva Metropolitan Area\": \"Switzerland\", \"Basel Metropolitan Area\": \"Switzerland\",\n",
    "    \"Zürich Metropolitan Area\": \"Switzerland\", \"Lugano Metropolitan Area\": \"Switzerland\",\n",
    "    \"Lausanne Metropolitan Area\": \"Switzerland\", \"Lucerne Metropolitan Area\": \"Switzerland\",\n",
    "    \"Sankt Gallen Metropolitan Area\": \"Switzerland\",\n",
    "    \"Greater Turin Metropolitan Area\": \"Italy\", \"Italy Metropolitan Area\": \"Italy\",\n",
    "    \"Greater Pavia Metropolitan Area\": \"Italy\", \"Greater Parma Metropolitan Area\": \"Italy\",\n",
    "    \"Greater Naples Metropolitan Area\": \"Italy\", \"Greater Bergamo Metropolitan Area\": \"Italy\",\n",
    "    \"Greater Brescia Metropolitan Area\": \"Italy\", \"Greater Verona Metropolitan Area\": \"Italy\",\n",
    "    \"Greater Rennes Metropolitan Area\": \"France\", \"Greater Grenoble Metropolitan Area\": \"France\",\n",
    "    \"Greater Bordeaux Metropolitan Area\": \"France\", \"Greater Montpellier Metropolitan Area\": \"France\",\n",
    "    \"Greater Strasbourg Metropolitan Area\": \"France\", \"Greater Toulouse Metropolitan Area\": \"France\",\n",
    "    \"Greater Nantes Metropolitan Area\": \"France\",\n",
    "    \"Greater Malmö Metropolitan Area\": \"Sweden\", \"Greater Gothenburg Metropolitan Area\": \"Sweden\",\n",
    "    \"Greater Helsingborg Metropolitan Area\": \"Sweden\", \"Greater Västerås Metropolitan Area\": \"Sweden\",\n",
    "    \"Tarnow Metropolitan Area\": \"Poland\", \"Cracow Metropolitan Area\": \"Poland\",\n",
    "    \"Kielce Metropolitan Area\": \"Poland\", \"Katowice Metropolitan Area\": \"Poland\",\n",
    "    \"Lodz Metropolitan Area\": \"Poland\", \"Poznan Metropolitan Area\": \"Poland\",\n",
    "    \"Zamosc Metropolitan Area\": \"Poland\",\n",
    "    \"Greater Palma de Mallorca Metropolitan Area\": \"Spain\", \"Greater Bilbao Metropolitan Area\": \"Spain\",\n",
    "    \"Greater Vigo Metropolitan Area\": \"Spain\", \"Greater Granada Metropolitan Area\": \"Spain\",\n",
    "    \"Greater Valencia Metropolitan Area\": \"Spain\",\n",
    "    \"Galway Metropolitan Area\": \"Ireland\",\n",
    "    \"Kortrijk Metropolitan Area\": \"Belgium\", \"Antwerp Metropolitan Area\": \"Belgium\",\n",
    "    \"Ghent Metropolitan Area\": \"Belgium\", \"Charleroi Metropolitan Area\": \"Belgium\",\n",
    "    \"Greater Nuremberg Metropolitan Area\": \"Germany\",\n",
    "    \"Pecs Metropolitan Area\": \"Hungary\",\n",
    "    \"Ljubljana Metropolitan Area\": \"Slovenia\",\n",
    "\n",
    "    # === NEW ONES YOU JUST POSTED ===\n",
    "    \"Hannover-Braunschweig-Göttingen-Wolfsburg Region\": \"Germany\",\n",
    "    \"Brabantine City Row\": \"Netherlands\",\n",
    "    \"Greater Dusseldorf Area\": \"Germany\",\n",
    "    \"Greater Mulhouse Area\": \"France\",\n",
    "    \"Linz-Wels-Steyr Area\": \"Austria\",\n",
    "    \"Greater Alicante Area\": \"Spain\",\n",
    "    \"Greater Funchal Area\": \"Portugal\",\n",
    "    \"Greater Ipswich Area\": \"United Kingdom\",\n",
    "    \"Greater La Coruña Area\": \"Spain\",\n",
    "    \"Eindhoven Area\": \"Netherlands\",\n",
    "    \"Greater Liverpool Area\": \"United Kingdom\",\n",
    "    \"Greater Kecskemet Area\": \"Hungary\",\n",
    "    \"Greater Leipzig Area\": \"Germany\",\n",
    "    \"Utrecht Area\": \"Netherlands\",\n",
    "    \"Greater Kiel Area\": \"Germany\",\n",
    "    \"Greater Lyon Area\": \"France\",\n",
    "    \"Greater Bielefeld Area\": \"Germany\",\n",
    "    \"Greater Avignon Area\": \"France\",\n",
    "    \"Greater Bern Area\": \"Switzerland\",\n",
    "    \"Greater Hamburg Area\": \"Germany\",\n",
    "    \"Greater Leeds Area\": \"United Kingdom\",\n",
    "    \"Greater Metz Area\": \"France\",\n",
    "    \"Greater Trencin Area\": \"Slovakia\",\n",
    "    \"European Economic Area\": \"European Economic Area\",\n",
    "    'Osnabrück Land': 'Germany',\n",
    "    \"Greater Salzburg\": \"Austria\",\n",
    "    \"Greater Sankt Polten\": \"Austria\",      # also works if written without ö\n",
    "    \"Greater Graz\": \"Austria\",\n",
    "    \"Greater Dublin\": \"Ireland\",\n",
    "    \"Greater Nottingham\": \"United Kingdom\",\n",
    "})\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Create a new column 'Country' by extracting the text after the last comma\n",
    "df[\"Country\"] = df[\"Location\"].apply(lambda x: x.split(\",\")[-1].strip() if \",\" in x else x.strip())\n",
    "\n",
    "# Exclude rows where Location contains \"EMEA\" or \"European Union\" (as these are not countries)\n",
    "df = df[~df[\"Location\"].str.contains(\"EMEA|European Union\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check remaining \"area\"\n",
    "country_df = df[df['Country'].str.contains('great', case=False, na=False)]\n",
    "country_df['Country']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ea733",
   "metadata": {},
   "source": [
    "#### Save the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce73e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('cleaned_linkedin_jobs.json', orient='records', lines=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cdada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_linkedin_jobs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
